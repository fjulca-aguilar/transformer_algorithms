import argparse
import datasets
from transformers import *

parser = argparse.ArgumentParser(description='Process some integers.')
parser.add_argument('--mode', type=str, default='train',
                    choices=['train', 'eval'])
parser.add_argument('--model_type', type=str, default='EDTransformer',
                    choices=['EDTransformer', 'DTransformer', 'ETransformer'])
parser.add_argument('--lrate', type=float,
                    default=1e-3,
                    help='Learning rate.')
parser.add_argument('--nEpochs', type=int,
                    default=3,
                    help='Number of training epochs.')
parser.add_argument('--model_path', type=str, default='EDTraining_model.pth',
                    help='Path to model to be saved during training or to be loaded during evaluation.')
parser.add_argument('--source', type=str, default='Go.',
                    help='Source to be processed (translated) by EDTransformer.')
parser.add_argument('--prompt', type=str, default='He had',
                    help='Prompt to be processed (continued) by DTransformer.')
parser.add_argument('--new_text_len', type=int,
                    default=10,
                    help='Number of characters to be generated by DTransformer.')

parser.add_argument('--max_len', type=int,
                    default=20,
                    help='Max sequence lenght.')

EDT_config ={
    'Lenc': 2, 
    'Ldec': 2, 
    'H': 2, 
    'de': 512, 
    'dmlp': 2048, 
    'dattn': 128, 
    'dmid': 128
}

DT_config ={
    'L': 2, 
    'H': 2, 
    'de': 512, 
    'dmlp': 2048, 
    'dattn': 128, 
    'dmid': 128
}

ET_config ={
    'L': 2, 
    'H': 2, 
    'de': 512, 
    'dmlp': 2048, 
    'dattn': 128, 
    'dmid': 128,
    'df': 128
}


def run_train(args):
    ds = datasets.Dataset()
    if args.model_type == 'EDTransformer':        
        ds.load_translation_dataset(max_len=args.max_len)
        transformer = EDTransformer(Nv=ds.vocabulary_size(), lmax=args.max_len, **EDT_config)
        EDTraining(transformer, ds.z, ds.x, lrate=args.lrate, 
                    nEpochs=args.nEpochs, model_path=args.model_path)
    else:
        ds.load_book_dataset(max_len=args.max_len)
        if args.model_type == 'DTransformer':            
            transformer = DTransformer(Nv=ds.vocabulary_size(), lmax=args.max_len, **DT_config)
            DTraining(transformer, ds.x, lrate=args.lrate, nEpochs=args.nEpochs,
                      model_path=args.model_path)
        else:
            transformer = ETransformer(Nv=ds.vocabulary_size(), lmax=args.max_len, **ET_config)
            ETraining(transformer, ds.x, lrate=args.lrate, 
                      mask_token=ds.detokenize([ds.mask_char])[0], 
                      nEpochs=args.nEpochs, model_path=args.model_path)


def run_eval(args):
    ds = datasets.Dataset()
    if args.model_type == 'EDTransformer':
        ds.load_translation_dataset(max_len=args.max_len)
        transformer = EDTransformer(Nv=ds.vocabulary_size(), lmax=args.max_len, **EDT_config)
        z = torch.tensor([ds.char2num[ds.bos_char]] + \
            ds.tokenize(args.source.lower()) + \
            [ds.char2num[ds.eos_char]])
        transformer.load_state_dict(torch.load(args.model_path))
        transformer.eval()
        x = EDInference(z, transformer, ds.char2num[ds.bos_char], ds.char2num[ds.eos_char])
        print(f"Text in Spanish: {''.join(ds.detokenize(x.numpy()))}")
    elif args.model_type == 'DTransformer':
        ds.load_book_dataset()
        transformer = DTransformer(Nv=ds.vocabulary_size(), lmax=args.max_len, **DT_config)
        x = torch.tensor(ds.tokenize(ds.bos_char + args.prompt.lower()))
        transformer.load_state_dict(torch.load(args.model_path))
        transformer.eval()
        x = DInference(x, args.new_text_len, transformer)
        print(f"New text: {''.join(ds.detokenize(x.numpy()))}")
    else:
        print('No eval/inference algorithm for ', args.model_type)


# TODO: improve initialization, too much variance 
# in between training experiments with same parameters.
if __name__ == '__main__':
    args = parser.parse_args()
    if args.mode == 'train':
        run_train(args)
    else:
        run_eval(args)
    